{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "hftoken = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(hftoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.10.3: Fast Qwen2 patching. Transformers = 4.44.2.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe MIG 3g.40gb. Max memory: 39.5 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.5.0+cu124. CUDA = 8.0. CUDA Toolkit = 12.4.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
      "Unsloth: Casting embed_tokens to float32\n",
      "Unsloth: Casting lm_head to float32\n",
      "trainable params: 1,412,956,160 || all params: 9,028,572,672 || trainable%: 15.6498\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "\n",
    "max_seq_length = 8192\n",
    "dtype = None # None으로 지정할 경우 해당 컴퓨팅 유닛에 알맞은 dtype으로 저장됩니다. Tesla T4와 V100의 경우에는 Float16, Ampere+ 이상의 경우에는 Bfloat16으로 설정됩니다.\n",
    "load_in_4bit = True # 메모리 사용량을 줄이기 위해서는 4bit 양자화를 사용하실 것을 권장합니다.\n",
    "\n",
    "# 모델 및 토크나이저 선언\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Qwen/Qwen2-7B-Instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # gated model을 사용할 경우 허깅페이스 토큰을 입력해주시길 바라겠습니다.\n",
    ")\n",
    "# LoRA Adapter 선언\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # 0을 넘는 숫자를 선택하세요. 8, 16, 32, 64, 128이 추천됩니다.\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "                      \"embed_tokens\", \"lm_head\",], # target module도 적절하게 조정할 수 있습니다.\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # 어떤 값이든 사용될 수 있지만, 0으로 최적화되어 있습니다.\n",
    "    bias = \"none\",    # 어떤 값이든 사용될 수 있지만, \"none\"으로 최적화되어 있습니다.\n",
    "    use_gradient_checkpointing = \"unsloth\", # 매우 긴 context에 대해 True 또는 \"unsloth\"를 사용하십시오.\n",
    "    random_state = 42,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None\n",
    ")\n",
    "prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "아래는 지시사항과 추가 정보가 포함된 내용입니다. 주어진 내용을 참고하여 한국어로 적절하게 답변해 주세요.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"question\"]\n",
    "    outputs = examples[\"response\"]\n",
    "    texts = []\n",
    "    for instruction, output in zip(instructions, outputs):\n",
    "        text = prompt_format.format(instruction, output) + EOS_TOKEN # 마지막에 eos token을 추가해줌으로써 모델이 출력을 끝마칠 수 있게 만들어 줍니다.\n",
    "        texts.append(text)\n",
    "    return { \"formatted_text\" : texts, }\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b93a5c1f53f47bc92307b77461e0098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/92 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55efe0c8afe643aeb22b6dc3a94e4e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4cbab408944fac912b24c05e0a9e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3871a6b524ea4b8ba77a4d96f0cb88e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b21bae02bf8407eba4761bc98fb7bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32e92defa63409ba79e882389554bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'response', 'formatted_text'],\n",
       "    num_rows: 25951\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset1 = load_dataset(\"llmflow/krx_dataset_by_gpt\", split='train')\n",
    "dataset2 = load_dataset(\"Linq-AI-Research/FinanceRAG\",'FinQABench', split = \"corpus\")\n",
    "dataset3 = load_dataset(\"amphora/rewrite-se-quant\")['train']\n",
    "dataset4 = load_dataset(\"amphora/krx-sample-instructions\", split='train')\n",
    "dataset5 = Dataset.from_pandas(pd.concat([\n",
    "    pd.read_csv('금융시장_100q.csv'),\n",
    "    pd.read_csv('재무회계_100q.csv'),\n",
    "    pd.read_csv('재무회계_100q2.csv'),\n",
    "    pd.read_csv('금융에이전트_100q.csv'),\n",
    "    pd.read_csv('주가예측_100q.csv')\n",
    "    ]))\n",
    "dataset6 = Dataset.from_pandas(pd.read_csv('ep_info.csv'))\n",
    "dataset7 = Dataset.from_pandas(pd.read_csv('rules.csv'))\n",
    "\n",
    "tdf = pd.DataFrame(dataset2)\n",
    "tdf = tdf[['title', 'text']]\n",
    "tdf.columns = ['question', 'response']\n",
    "dataset2 = Dataset.from_pandas(tdf)\n",
    "\n",
    "tdf = pd.DataFrame(dataset3)\n",
    "tdf = tdf[['query', 'output']]\n",
    "tdf.columns = ['question', 'response']\n",
    "dataset3 = Dataset.from_pandas(tdf)\n",
    "\n",
    "tdf = pd.DataFrame(dataset4)\n",
    "tdf = tdf[['prompt', 'response']]\n",
    "tdf.columns = ['question', 'response']\n",
    "dataset4 = Dataset.from_pandas(tdf)\n",
    "\n",
    "tdf = pd.DataFrame(dataset6)\n",
    "tdf = tdf[['Title', 'Context']]\n",
    "tdf.columns = ['question', 'response']\n",
    "dataset6 = Dataset.from_pandas(tdf)\n",
    "\n",
    "tdf = pd.DataFrame(dataset7)\n",
    "tdf = tdf[['Title', 'Context']]\n",
    "tdf.columns = ['question', 'response']\n",
    "dataset7 = Dataset.from_pandas(tdf)\n",
    "\n",
    "dataset1 = dataset1.map(formatting_prompts_func, batched = True,)\n",
    "dataset2 = dataset2.map(formatting_prompts_func, batched = True,)\n",
    "dataset3 = dataset3.map(formatting_prompts_func, batched = True,)\n",
    "dataset4 = dataset4.map(formatting_prompts_func, batched = True,)\n",
    "dataset5 = dataset5.map(formatting_prompts_func, batched = True,)\n",
    "dataset6 = dataset6.map(formatting_prompts_func, batched = True,)\n",
    "dataset7 = dataset7.map(formatting_prompts_func, batched = True,)\n",
    "\n",
    "# 데이터셋 통합\n",
    "# combined_dataset = concatenate_datasets([dataset4, dataset5, dataset6, dataset7])\n",
    "combined_dataset = dataset4\n",
    "\n",
    "combined_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n아래는 지시사항과 추가 정보가 포함된 내용입니다. 주어진 내용을 참고하여 한국어로 적절하게 답변해 주세요.\\n\\n### Instruction:\\n음악산업의 디지털화가 진행됨에 따라, 전통적인 음반 중심의 수익 모델에서 디지털 음악 서비스 중심으로 변화했음을 설명하는데, 이러한 변화가 이루어진 주된 원인은 무엇이며, 이에 따라 업계가 어떻게 대응하고 있는지 구체적인 예를 들어 설명하시오.\\n\\n### Response:\\n음악산업의 디지털화 주된 원인은 인터넷과 мобиль 서비스의 발달로 인해 소비자들이 음악을 접근하고 소비하는 방식이 변화했기 때문입니다. 디지털 플랫폼의 등장으로 음원 스트리밍 서비스(예: 멜론, 스포티파이 등)가 보편화되면서, 소비자들은 과거의 음반 구매 대신 구독 기반의 서비스를 통해 음악에 쉽게 접근하게 되었습니다.\\n\\n이에 따라 음악업계는 다양한 대응 전략을 마련하고 있습니다. 예를 들어, 아티스트들은 음원 스트리밍 수익을 최적화하기 위해 SNS와 유튜브를 활용한 마케팅을 강화하고 있으며, 라이브 공연 및 팬미팅과 같은 오프라인 이벤트를 확대하여 추가 수익을 창출하고 있습니다. 또한, 일부 음악 레이블은 아티스트와의 협업을 통해 독점 콘텐츠를 제작하고, 메타버스와 같은 새로운 플랫폼을 이용해 가상 공연을 개최하는 등의 혁신적인 접근법을 시도하고 있습니다.<|im_end|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(combined_dataset)['formatted_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25951\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c8241ce4364e999ac65da3230b1132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/25951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n아래는 지시사항과 추가 정보가 포함된 내용입니다. 주어진 내용을 참고하여 한국어로 적절하게 답변해 주세요.\\n\\n### Instruction:\\n다음 문제를 읽고 정답으로 가장 알맞은 것을 고르시요.\\n### 질문: 다음 중 우리나라 주식시장 매매 제도에 대한 기술로 맞는 것은?\\n### 선택지: \\nA. 개장 시간은 오전 10시다.\\nB. 유가증권시장의 가격 제한폭은 전일 종가 대비 상하 15%이다.\\nC. 코스닥시장에는 가격 제한폭이 없다.\\nD. 점심시간(12~1시)에는 휴장한다.\\nE. 동시호가는 폐장 시간에만 적용한다.\\nF. K-OTC시장의 가격 제한폭은 전일 종가 대비 상하 30%이다.\\nG. K-OTC시장의 운영시간은 09:00부터 16:00까지이다.\\n\\n### Response:\\n본 질문의 정답을 찾기 위해 각 선택지들을 살펴봅니다.\\n\\nA. 개장 시간은 오전 10시다.\\n이 문장은 주식 시장의 개장 시간에 대한 정보이며, 일반적으로 주식 시장의 개장 시간은 이와 같으므로 올바른 정보라고 판단됩니다.\\n\\nB. 유가증권시장의 가격 제한폭은 전일 종가 대비 상하 15%이다.\\n이 문장은 주식 시장의 가격 제한폭에 대한 정보이며, 유가증권시장의 가격 제한폭이 전일 종가 대비 상하 15%라는 정보는 일반적으로 주식 시장의 규칙에 따른 정보라고 판단됩니다.\\n\\nC. 코스닥시장에는 가격 제한폭이 없다.\\n코스닥시장에 대한 정보입니다. 실제로 코스닥시장에는 가격 제한폭이 있으므로 이 문장은 잘못된 정보라고 판단됩니다.\\n\\nD. 점심시간(12~1시)에는 휴장한다.\\n이 문장은 주식 시장의 휴장 시간에 대한 정보이며, 대부분의 주식 시장에서 점심시간에는 휴장하지 않는다는 정보가 일반적입니다. 따라서 이 문장은 잘못된 정보라고 판단됩니다.\\n\\nE. 동시호가는 폐장 시간에만 적용한다.\\n이 문장은 주식 시장의 동시호가 제한에 대한 정보입니다. 동시호가 제한은 주로 폐장 시간에 적용되는 것으로 알려져 있으므로 이 문장은 올바른 정보라고 판단됩니다.\\n\\nF. K-OTC시장의 가격 제한폭은 전일 종가 대비 상하 30%이다.\\n이 문장은 주식 시장의 가격 제한폭에 대한 정보이며, K-OTC시장의 가격 제한폭이 전일 종가 대비 상하 30%라는 정보는 일반적으로 주식 시장의 규칙에 따른 정보라고 판단됩니다.\\n\\nG. K-OTC시장의 운영시간은 09:00부터 16:00까지이다.\\n이 문장은 주식 시장의 운영시간에 대한 정보이며, K-OTC시장의 운영시간이 09:10부터 16:10까지라는 정보는 일반적으로 주식 시장의 규칙에 따른 정보라고 판단됩니다.\\n\\n따라서, 가장 알맞은 정답을 고르면 A, B, E, F, G가 해당될 수 있습니다. 이들 중에서 가장 명확한 정답을 고르면 A와 E가 가장 명확한 정답으로 보입니다. 그러나 주어진 질문에 대한 정확한 답을 제공하기 위해서는 추가 정보나 구체적인 맥락이 필요합니다. 이 질문의 경우, 여러 가지 정보들이 혼합되어 있으며, 어느 한 가지 선택지가 모든 정보를 반영하는 것은 아닙니다. 따라서 이 질문에 대한 가장 정확한 답변을 제공하기 위해서는 추가 정보나 구체적인 맥락이 필요합니다.<|im_end|>']\n",
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "아래는 지시사항과 추가 정보가 포함된 내용입니다. 주어진 내용을 참고하여 한국어로 적절하게 답변해 주세요.\n",
      "\n",
      "### Instruction:\n",
      "다음 문제를 읽고 정답으로 가장 알맞은 것을 고르시요.\n",
      "### 질문: 다음 중 우리나라 주식시장 매매 제도에 대한 기술로 맞는 것은?\n",
      "### 선택지: \n",
      "A. 개장 시간은 오전 10시다.\n",
      "B. 유가증권시장의 가격 제한폭은 전일 종가 대비 상하 15%이다.\n",
      "C. 코스닥시장에는 가격 제한폭이 없다.\n",
      "D. 점심시간(12~1시)에는 휴장한다.\n",
      "E. 동시호가는 폐장 시간에만 적용한다.\n",
      "F. K-OTC시장의 가격 제한폭은 전일 종가 대비 상하 30%이다.\n",
      "G. K-OTC시장의 운영시간은 09:00부터 16:00까지이다.\n",
      "\n",
      "### Response:\n",
      "본 질문의 정답을 찾기 위해 각 선택지들을 살펴봅니다.\n",
      "\n",
      "A. 개장 시간은 오전 10시다.\n",
      "이 문장은 주식 시장의 개장 시간에 대한 정보이며, 일반적으로 주식 시장의 개장 시간은 이와 같으므로 올바른 정보라고 판단됩니다.\n",
      "\n",
      "B. 유가증권시장의 가격 제한폭은 전일 종가 대비 상하 15%이다.\n",
      "이 문장은 주식 시장의 가격 제한폭에 대한 정보이며, 유가증권시장의 가격 제한폭이 전일 종가 대비 상하 15%라는 정보는 일반적으로 주식 시장의 규칙에 따른 정보라고 판단됩니다.\n",
      "\n",
      "C. 코스닥시장에는 가격 제한폭이 없다.\n",
      "코스닥시장에 대한 정보입니다. 실제로 코스닥시장에는 가격 제한폭이 있으므로 이 문장은 잘못된 정보라고 판단됩니다.\n",
      "\n",
      "D. 점심시간(12~1시)에는 휴장한다.\n",
      "이 문장은 주식 시장의 휴장 시간에 대한 정보이며, 대부분의 주식 시장에서 점심시간에는 휴장하지 않는다는 정보가 일반적입니다. 따라서 이 문장은 잘못된 정보라고 판단됩니다.\n",
      "\n",
      "E. 동시호가는 폐장 시간에만 적용한다.\n",
      "이 문장은 주식 시장의 동시호가 제한에 대한 정보입니다. 동시호가 제한은 주로 폐장 시간에 적용되는 것으로 알려져 있으므로 이 문장은 올바른 정보라고 판단됩니다.\n",
      "\n",
      "F. K-OTC시장의 가격 제한폭은 전일 종가 대비 상하 30%이다.\n",
      "이 문장은 주식 시장의 가격 제한폭에 대한 정보이며, K-OTC시장의 가격 제한폭이 전일 종가 대비 상하 30%라는 정보는 일반적으로 주식 시장의 규칙에 따른 정보라고 판단됩니다.\n",
      "\n",
      "G. K-OTC시장의 운영시간은 09:00부터 16:00까지이다.\n",
      "이 문장은 주식 시장의 운영시간에 대한 정보이며, K-OTC시장의 운영시간이 09:10부터 16:10까지라는 정보는 일반적으로 주식 시장의 규칙에 따른 정보라고 판단됩니다.\n",
      "\n",
      "따라서, 가장 알맞은 정답을 고르면 A, B, E, F, G가 해당될 수 있습니다. 이들 중에서 가장 명확한 정답을 고르면 A와 E가 가장 명확한 정답으로 보입니다. 그러나 주어진 질문에 대한 정확한 답을 제공하기 위해서는 추가 정보나 구체적인 맥락이 필요합니다. 이 질문의 경우, 여러 가지 정보들이 혼합되어 있으며, 어느 한 가지 선택지가 모든 정보를 반영하는 것은 아닙니다. 따라서 이 질문에 대한 가장 정확한 답변을 제공하기 위해서는 추가 정보나 구체적인 맥락이 필요합니다.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(len(combined_dataset))\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = combined_dataset,\n",
    "    dataset_text_field = \"formatted_text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 4,\n",
    "    packing = False, # True로 설정하면 짧은 텍스트 데이터에 대해서는 더 빠른 학습 속도로를 보여줍니다.\n",
    "    args = TrainingArguments( # TrainingArguments는 자신의 학습 환경과 기호에 따라 적절하게 설정하면 됩니다.\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1,\n",
    "        max_steps = 100,\n",
    "        learning_rate = 1e-6,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 42,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    prompt_format.format(\n",
    "        \"\"\"다음 문제를 읽고 정답으로 가장 알맞은 것을 고르시요.\n",
    "### 질문: 다음 중 우리나라 주식시장 매매 제도에 대한 기술로 맞는 것은?\n",
    "### 선택지: \n",
    "A. 개장 시간은 오전 10시다.\n",
    "B. 유가증권시장의 가격 제한폭은 전일 종가 대비 상하 15%이다.\n",
    "C. 코스닥시장에는 가격 제한폭이 없다.\n",
    "D. 점심시간(12~1시)에는 휴장한다.\n",
    "E. 동시호가는 폐장 시간에만 적용한다.\n",
    "F. K-OTC시장의 가격 제한폭은 전일 종가 대비 상하 30%이다.\n",
    "G. K-OTC시장의 운영시간은 09:00부터 16:00까지이다.\"\"\", # instruction\n",
    "        \"\", # output 생성을 위해 빈 칸으로 설정\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "for item in tokenizer.batch_decode(outputs):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = llmflow.\n",
      "We shall truncate llmflow/krx_Qwen2_7B_Instruct_v2 to krx_Qwen2_7B_Instruct_v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 29.79 out of 48.0 RAM for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [00:01<00:00, 20.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n",
      "Unsloth: Saving model... This might take 5 minutes for Llama-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018699095ea941dc9fe0f4b260559995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/588 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6480c9432c408194e23feed28cb05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa7385494b64fbbaf245787f86bb6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ef3f36d7944aad908b8dba909251ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f25488e9054a11b44bcd4c6b7a06ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f277c3e630b498ba842336ed51adb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/llmflow/krx_Qwen2_7B_Instruct_v2\n"
     ]
    }
   ],
   "source": [
    "# LoRA Adapter 저장\n",
    "# model.save_pretrained(\"lora_model\")\n",
    "# tokenizer.save_pretrained(\"lora_model\")\n",
    "\n",
    "# # Merged model 저장 및 업로드\n",
    "# model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "model.push_to_hub_merged(\"llmflow/krx_Qwen2_7B_Instruct_v2\", tokenizer, save_method = \"merged_16bit\", token = hftoken) # 개인 huggingface token을 사용하여 업로드할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
